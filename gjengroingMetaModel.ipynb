{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script takes the output tiles from gjengroingMetaModelDownloadImageTiles.Rmd and passes them to the meta model for canopy height inference. Please see https://github.com/facebookresearch/HighResCanopyHeight for information on installing model dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import torch.nn as nn\n",
    "import rasterio\n",
    "import torch.nn.functional as F\n",
    "from models.backbone import SSLVisionTransformer\n",
    "from models.dpt_head import DPTHead\n",
    "import pytorch_lightning as pl\n",
    "from models.regressor import RNet\n",
    "import torchvision.transforms.functional as TF\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we have a function that blends areas in overlapping regions of the tiles. If your output has strong edge effects, try adding overlap when downloading the tiles. This function reduces these edge effects by ensuring adjacent tiles are better matched after normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blend_overlap(region1, region2, weight=0.5):\n",
    "    return (weight * region1 + (1 - weight) * region2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step performs image normalisation. First tiles are passed to a pretrained model that modifies the aerial image histogram to match that of Maxar imagery (what the model was trained on) and then performs a normalisation step using precalculated band averages and standard deviation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_image(image, model_norm, device, overlap=None, weight=0.5):\n",
    "    if image.shape[0] != 3:\n",
    "        raise ValueError(f\"Expected 3-channel image, but got {image.shape[0]} channels.\")\n",
    "\n",
    "    x = torch.unsqueeze(image, dim=0).to(device)\n",
    "\n",
    "    # Use model to predict p5 and p95 values\n",
    "    norm_img = model_norm(x).detach()\n",
    "\n",
    "    p5I = [norm_img[0][0].item(), norm_img[0][1].item(), norm_img[0][2].item()]\n",
    "    p95I = [norm_img[0][3].item(), norm_img[0][4].item(), norm_img[0][5].item()]\n",
    "\n",
    "    p5In = [np.percentile(image[i, :, :].numpy(), 20) for i in range(3)]\n",
    "    p95In = [np.percentile(image[i, :, :].numpy(), 80) for i in range(3)]\n",
    "\n",
    "    normIn = image.clone()\n",
    "    \n",
    "    for i in range(3):\n",
    "        normIn[i, :, :] = (image[i, :, :] - p5In[i]) * ((p95I[i] - p5I[i]) / (p95In[i] - p5In[i])) + p5I[i]\n",
    "    \n",
    "    # If overlap region is provided, blend the overlap\n",
    "    if overlap is not None:\n",
    "        for i in range(3):\n",
    "            overlap_region = overlap[i, :, :]\n",
    "            normIn[i, :, :] = blend_overlap(normIn[i, :, :], overlap_region, weight=weight)\n",
    "\n",
    "    return normIn\n",
    "\n",
    "# Apply final normalization using the general image normalization values\n",
    "def final_normalize(image, norm_values):\n",
    "    norm = T.Normalize(*norm_values)\n",
    "    return norm(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSLAE(nn.Module):\n",
    "    def __init__(self, pretrained=None, classify=True, n_bins=256, huge=False):\n",
    "        super().__init__()\n",
    "        if huge:\n",
    "            self.backbone = SSLVisionTransformer(\n",
    "                embed_dim=1280,\n",
    "                num_heads=20,\n",
    "                out_indices=(9, 16, 22, 29),\n",
    "                depth=32,\n",
    "                pretrained=pretrained\n",
    "            )\n",
    "            self.decode_head = DPTHead(\n",
    "                classify=classify,\n",
    "                in_channels=(1280, 1280, 1280, 1280),\n",
    "                embed_dims=1280,\n",
    "                post_process_channels=[160, 320, 640, 1280],\n",
    "            )\n",
    "        else:\n",
    "            self.backbone = SSLVisionTransformer(pretrained=pretrained)\n",
    "            self.decode_head = DPTHead(classify=classify, n_bins=256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.decode_head(x)\n",
    "        return x\n",
    "\n",
    "class SSLModule(pl.LightningModule):\n",
    "    def __init__(self, ssl_path=\"compressed_SSLbaseline.pth\"):\n",
    "        super().__init__()\n",
    "\n",
    "        if 'huge' in ssl_path:\n",
    "            self.chm_module_ = SSLAE(classify=True, huge=True).eval()\n",
    "        else:\n",
    "            self.chm_module_ = SSLAE(classify=True, huge=False).eval()\n",
    "\n",
    "        if 'compressed' in ssl_path:\n",
    "            ckpt = torch.load(ssl_path, map_location='cpu')\n",
    "            self.chm_module_ = torch.quantization.quantize_dynamic(\n",
    "                self.chm_module_,\n",
    "                {torch.nn.Linear, torch.nn.Conv2d, torch.nn.ConvTranspose2d},\n",
    "                dtype=torch.qint8)\n",
    "            self.chm_module_.load_state_dict(ckpt, strict=False)\n",
    "        else:\n",
    "            ckpt = torch.load(ssl_path)\n",
    "            state_dict = ckpt['state_dict']\n",
    "            self.chm_module_.load_state_dict(state_dict)\n",
    "\n",
    "        self.chm_module = lambda x: 9.51 * self.chm_module_(x) + 0.4\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.chm_module(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function sets the model up with the tiles we downloaded with the R script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, model_norm, norm_values, name, bs=32, device='cuda:0', display=False):\n",
    "    ds = NorwayDataset(src_img_dir='data/encroachment/metamodel/tiles')\n",
    "    dataloader = torch.utils.data.DataLoader(ds, batch_size=bs, shuffle=False, num_workers=8, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "    for batch in dataloader:\n",
    "        images, img_paths = batch\n",
    "\n",
    "        # Convert list of images to a single tensor\n",
    "        images = torch.stack(images).to(device)\n",
    "\n",
    "        # Ensure the dimensions are in the correct order (channels first)\n",
    "        images = images.permute(0, 2, 3, 1)  # From [batch, height, width, channels] to [batch, channels, height, width]\n",
    "\n",
    "        # Step 1: Apply histogram normalization (matching satellite images)\n",
    "        images = torch.stack([normalize_image(img, model_norm, device) for img in images])\n",
    "\n",
    "        # Step 2: Apply final normalization (standard image normalization)\n",
    "        images = torch.stack([final_normalize(img, norm_values) for img in images])\n",
    "\n",
    "        pred = model(images)\n",
    "        pred = pred.cpu().detach().relu()\n",
    "\n",
    "        for ind in range(pred.shape[0]):\n",
    "            img_path = img_paths[ind]\n",
    "\n",
    "            with rasterio.open(img_path) as src:\n",
    "                meta = src.meta.copy()\n",
    "                transform = src.transform  # Save the transform to preserve the location\n",
    "                crs = src.crs  # Save the CRS\n",
    "\n",
    "            # Update meta to reflect the number of layers\n",
    "            meta.update(dtype=rasterio.float32, count=1)\n",
    "\n",
    "            # Save the prediction as GeoTIFF\n",
    "            output_path = Path(name) / f'{img_path.stem}_pred.tif'\n",
    "            with rasterio.open(output_path, 'w', **meta) as dst:\n",
    "                dst.write(pred[ind].numpy()[0], 1)  # Writing the single prediction band\n",
    "                dst.transform = transform  # Ensure the correct geotransform is used\n",
    "                dst.crs = crs  # Ensure the correct CRS is used\n",
    "\n",
    "            if display:\n",
    "                plt.imshow(pred[ind][0].numpy())\n",
    "                plt.title(f'Prediction for {img_path.name}')\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This defines the dataset to be passed to the model and removes null tiles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NorwayDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, src_img_dir='data/encroachment/metamodel/tiles'):\n",
    "        self.src_img_dir = Path(src_img_dir)\n",
    "        self.img_files = list(self.src_img_dir.glob('*.tif'))  # Adjust extension if needed\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        img_path = self.img_files[i]\n",
    "        \n",
    "        # Use rasterio to read the GeoTIFF\n",
    "        with rasterio.open(img_path) as src:\n",
    "            img = TF.to_tensor(src.read([1, 2, 3]))  # Read the first 3 channels (assuming it's RGB)\n",
    "        \n",
    "        # Check if the image is all zeros\n",
    "        if torch.all(img == 0):\n",
    "            return self.__getitem__((i + 1) % len(self.img_files))  # Get the next image\n",
    "\n",
    "        return img, img_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sets the model arguments and runs it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='test a model')\n",
    "    parser.add_argument('--checkpoint', type=str, help='CHM pred checkpoint file', default='data/encroachment/metamodel/saved_checkpoints/compressed_SSLhuge_aerial.pth')\n",
    "    parser.add_argument('--name', type=str, help='run name', default='data/encroachment/metamodel/output_inference')\n",
    "    parser.add_argument('--display', type=bool, help='saving outputs in images')\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    if 'compressed' in args.checkpoint:\n",
    "        device = 'cpu'\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "\n",
    "    os.makedirs(args.name, exist_ok=True)\n",
    "\n",
    "    # Load SSL model\n",
    "    model = SSLModule(ssl_path=args.checkpoint)\n",
    "    model.to(device)\n",
    "    model = model.eval()\n",
    "\n",
    "    # Load normalization model (RNet for histogram matching)\n",
    "    norm_path = 'data/encroachment/metamodel/saved_checkpoints/aerial_normalization_quantiles_predictor.ckpt'\n",
    "    ckpt = torch.load(norm_path, map_location=device)\n",
    "    model_norm = RNet(n_classes=6).to(device).eval()\n",
    "    state_dict = ckpt['state_dict']\n",
    "    for k in list(state_dict.keys()):\n",
    "        if 'backbone.' in k:\n",
    "            new_k = k.replace('backbone.', '')\n",
    "            state_dict[new_k] = state_dict.pop(k)\n",
    "    model_norm.load_state_dict(state_dict)\n",
    "\n",
    "    # Image normalization values\n",
    "    norm_values = ((0.420, 0.411, 0.296), (0.213, 0.156, 0.143))\n",
    "\n",
    "    evaluate(model=model, model_norm=model_norm, norm_values=norm_values, name=args.name, bs=16, device=device, display=args.display)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
